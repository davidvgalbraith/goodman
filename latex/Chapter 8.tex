\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\include{davemathshortcuts}
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

\title{Chapter 8}
\author{Dave}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\section{The Idea of a Module}
\be
\item \be
\item Let $\psi$ be the map defining $M$ as an $R$-module. Then for $r \in R$, $\psi(r)$ is a homomorphism on $M$ so $r0 = 0$. And $\psi$ is a homomorphism from $R$ to End($M$), so $\psi(0) = 0$.
\item As in part a, this is a consequence of the homomorphism property of $\psi$ and $\psi(r)$ for all $r$.
\item As in part a, this is a consequence of the homomorphism property of $\psi$ and $\psi(r)$ for all $r$.
\ee
\item Let $W$ be a linear subspace of $V$ that is closed under $T$, that means that for all $w \in W$, $T(w) \in W$, and for all $k \in K, kw \in W$, so combining these facts and iterating we get that for all $p = \sum_ik_ix^i \in K[x], p(T)(w) \in W$, so $W$ is a $K[x]$ submodule of $V$. So those conditions are sufficient to define a $K[x]$ submodule, and it is readily apparent that they are necessary. For any $w \in W$, if $T(w) = 0$, then $p(T)(w) = \sum_ia_i0^i = 0$ for all $p \in K[x]$, so the colonel of $T$ is a submodule. And if $v \in T(V)$, then $v \in V \ra T(v) \in T(V) \ra p(T)(v) \in V \forall p$, so $T(V)$ is a submodule.
\item Let $p \in R$, then $p\sum_ir_ix_i = \sum pr_ix_i \in IM$ since $I$ is a left ideal so $pr_i \in I \forall i$.
\item Let $n \in N, r \in \ann(N), p \in R$, then $prn = p0 = 0$ and $rpn = rm$ for some $m \in N$ since $N$ is a submodule, so $rm = 0$ since $r$ annihilates $N$, so ann($N)$ is an ideal of $R$.
\item \be
\item Let $r \in R, x \in \cap_\alpha M_\alpha$, then $x \in M_\alpha \forall \alpha \ra rx \in M_\alpha \forall \alpha$ since each $M_\alpha$ is a submodule. So $rx \in \cap_\alpha M_\alpha$ so $\cap_\alpha M_\alpha$ is a submodule.
\item Let $x \in \cup_nM_n$ then $x \in M_k$ for some $k$ and all $m \ge k$ so $rx \in M_m$ for all such $m$ for all $r \in R$ since each $M_m$ is a submodule so $rx \in \cup_n M_n$ so $\cup_n M_n$ is a submodule.
\item $r(a + b) = ra + rb \in A + B$ since $A$ and $B$ are submodules.
\ee
\item \be
\item Let $p \in R$, then $p\sum_ir_is_i = \sum pr_is_i \in RS$ since $R$ is a left ideal of itself so $pr_i \in R \forall i$.
\item It's pretty clear that $\angles{S} + RS$ contains $S$ and it's a submodule since for any $p \in R$, $s + rt \in \angles{S} + RS$, we have $p(s + rt) = ps + prt \in RS$.
\item Any submodule containing $S$ must contain $\angles{S}$, since it has to be a group, and it must contain $RS$, since it's a submodule, and so it must contain $\angles{S} + RS$ since it's a group. Since we already established that $\angles{S} + RS$ is a submodule, this means it's minimal.
\item If $1 \in R$, then $1S = S$, so $S \subset RS$. And if $s, t \in S$, then $s + t = 1s + 1t \in RS$, and $-s = -1s \in RS$, so $\angles{S} \subset RS$, so $\angles{S} + RS = RS$.
\ee
\item Let $x_1$ and $x_2$ be linear independent vectors in a basis of $V$ over $K$. Then $f: \sum_ia_ix_i \to a_1x_2 + a_2x_1 + \sum_{i>2}a_ix_i$ is a nonzero endomorphism on $V$, but $x_1 - f(x_2) = 0$, so $x_1$ and $x_2$ are not linear independent over End$_K(V)$.
\item The elements of this group are $n$-tuples of $n$-dimensional vectors, e.g. if $V = \ints$ and $n=2$ then a representative element would be $([1, 2], [3, 4])$ and so you can arrange those vectors into the columns of a matrix and represent an endomorphism as a matrix multiplication to see that the group is isomorphic to the group of $n\times n$ matrices, which is generated by multiplying the identity matrix (which would be represented in $V^n$ by the vectors of the elementary basis of $V$) by all the matrices in Mat$_n(K)$.
\item Let $x \in V$ then for some $i$, $T^i = \sum_{j < i}a_jT^j$, for otherwise all the powers of $T(x)$ would be linear independent and hence form an infinite-dimensional basis for $V$ which we said it didn't have. So the polynomial $p(x) = x^i - \sum_{j < i}a_jx^j$ is nonzero and has the feature $p(T)(x) = 0$, so $\{x\}$ is not linear independent so $V$ is not free.
\item Suppose that conditions a and b hold, then by condition a, any $m \in M$ is equal to $\sum_i a_ix_i$, and by b, $f: (a_1, a_2, \ldots a_n) \to \sum_ia_ix_i$ is an isomorphism from $R^n$ to $M$, so $M = \oplus_iRx_i$. And if it were not the case that $r \to rx_i$ were injective, then there would exist $r, p \in R$ such that $rx_i = px_i$, so supposing without loss of generality that $i = 1$, we'd have $f(p, a_2, a_3, \ldots a_n) = px_1 + \sum_{i > 1}a_ix_i = rx_1 + \sum_{i > 1}a_ix_i = f(r, a_2, a_3, \ldots a_n)$, so $f$ would not be a bijection so not an isomorphism.

Suppose on the other hand that c holds, then each $m\in M$ is equal to $\sum_ia_im_i$, and each $a_i$ is uniquely determined by the injectivity of $r \to rx_i$, so the same $f$ is again an isomorphism.  
\item $\psi$ is a homomorphism of abelian groups $\lra \psi(a + a') = \psi(a) + \psi(a') \forall a, a' \in A \lra \psi(a + a + a + \ldots + a) = \psi(a) + \psi(a) + \ldots + \psi(a) \lra \psi(na) = n\psi(a) \forall n \in \ints \lra \psi$ is a $\ints$-module homomorphism from $A$ to $B$. 
\item Let $rx = (rx_1,rx_2 \ldots)$ for $r \in R, x \in R^\infty$, and let $x + y = (x_1 + y_1, x_2 + y_2, \ldots).$ Then for $x_i, y_i, r, s \in R$, $(rsx_1, rsx_2, \ldots) = r(sx_1, sx_2, \ldots)$;

 $(r + s)(x_1, x_2, \ldots) = ((r + s)x_1, (r + s)x_2, \ldots) = (rx_1 + sx_1, rs_2 + sx_2, \ldots) = (rx_1, rx_2, \ldots) + (sx_1, sx_2, \ldots) = r(x_1, x_2, \ldots) + s(x_1, x_2, \ldots)$; 
 
 and $r((x_1, x_2, \ldots) + (y_1, y_2, \ldots)) = r(x_1 + y_1, x_2+ y_2, \ldots) = (r(x_1 + y_1), r(x_2 + y_2), \ldots) = (r(x_1, x_2, \ldots) + r(y_1, y_2, \ldots))$, so $R^\infty$ is an $R$-module. It's free because it has the basis $\{(1, 0, 0, \ldots), (0, 1, 0, 0, \ldots), (0, 0, 1, 0, \ldots), \ldots\}$, which is countably infinite.
\ee
\section{Homomorphisms and Quotient Modules}
\be
\item \be
\item Let $x \in \ker f, r \in R$, then $\psi(rx) = r\psi(x) = r0 = 0$, and if $y \in f(M)$ with $y = f(x)$ then $ry = rf(x) = f(rx) \in f(M)$ since $M$ is a submodule, so the image and colonel of $\psi$ are submodules.
\item Let $a \in M, r \in R, f, g \in \homrmn$, then we showed a long time ago that $fg$ is a homomorphism of abelian groups, and furthermore $fg(ra) =f(g(ra)) = f(rg(a)) = rfg(a)$.
\ee
\item \be
\item For $\psi, \phi \in \homrmn, m, n \in M, -\psi(m) = \psi(-m) \in \homrmn$, $(\psi + \phi)(m + n) = (\psi + \phi)(m) + (\psi + \phi)(n) = \psi(m) + \phi(m) + \psi(n) + \phi(n) = (\psi + \phi)(m) + (\psi + \phi)(n)$, and $\psi(x) = 0 \in \homrmn$, so $\homrmn$ is a group. 
\item Composition is associative and distributive over addition, so it's a valid ring product.
\ee
\item Let $m + IM \in M/IM, r + I \in R/I$, then $(r + I)(m + IM) = rm + IM$ defines a module action on $M/IM$, since it's distributive and associative (and well-defined). 
\item For $r \in R, n \in N$, $r\psi\inv(n) = \psi\inv(rn) \in \psi\inv(N)$ since $R$ is a submodule, so $\psi$ carries submodules to submodules and so is a bijection between them, being already a bijection of abelian groups.
\item Let $f(m + N) = \psi(m) + \bar{N}, r \in R$, then $f$ is a group isomorphism and $f(rm + N) = \psi(rm) + \bar{N} = r\psi(m) + \bar{N} = r\psi(m + N)$, so $f$ is a module isomorphism.
\item Let $\tilde{\psi}(m + N) = \psi(m)$, then $\tilde{\psi}(rm+N) = \psi(rm) = r\psi(m) = r\tilde{\psi}(m + N)$, so $\tilde{\psi}$ is a module homomorphism. 
\item $\psi(A) \subset \psi(A + N)$ since $0 \in N$, and for $a \in A, n \in N$, $\psi(a + n) = \psi(a) + \psi(n) = \psi(a) + 0 = \psi(a) \in \psi(A)$, so $\psi(A + N) \subset \psi(A) \ra \psi(A + N) = \psi(A) \ra A + N \subset \psi\inv(\psi(A))$. And $x \in \psi\inv(\psi(A)) \ra \psi(x) \in \psi(A) \ra \psi(x) = \psi(a) = \psi(a) + 0$ for some $a \in A$, so $x = a + n$ for some $n \in N$. So $\psi\inv(\psi(A)) = A + N$, and $a \to \psi(a)$ is a map from $A$ to $\psi(A)$ with colonel $A \cap N$, and $a + n \to \psi(A)$ is a map from $A + N$ to $\psi(A)$ with colonel $N$. 
\item There exists a finite set $S = \{x_1, x_2, \ldots x_n\}$ such that $RS = M$, so let $F = \oplus_i Rx_i$, then $F$ is a free $R$-module, and the map $(r_1x_1, r_2x_2, \ldots r_nx_n) \to \sum_i r_ix_i$ is a surjective homomorphism from $F$ to $M$, and whatever its colonel $K$ is, it's a submodule of $F$ such that $M \cong F / K$.
\ee
\section{Multilinear Maps and Determinants}
\be
\item Let $f$ and $g$ be multilinear maps, then $(f + g)(\multilinear{x} + \multilinear{y}) = f(\multilinear{x + y}) + g(\multilinear{x + y}) = f(\multilinear{x}) + f(\multilinear{y}) + g(\multilinear{x}) + g(\multilinear{y}) = (f + g)(\multilinear{x}) + (f + g)(\multilinear{y})$, so $f + g$ is multilinear, and $-f$ and 0 are also multilinear, so the multilinear maps are a group, and they're abelian because $R$ is.
\item The definition of multilinearity is independent of the ordering of the groups $M_i$, so permuting them doesn't change the multilinearity of a function. The action of a permutation on a symmetric multilinear function results in just that same function, so the result is multilinear and symmetric. The action of a permutation on a skew-symmetric multilinear function $f$ returns $\pm f$, both of which are skew-symmetric. Finally, if $x_i = x_j$ for some $i, j$, then $x_{\sigma(i)} = x_{\sigma(j)}$ for any $\sigma$, so if $f$ is alternating then $\sigma f$ is alternating.
\item \be
\item Subsets of $R^n$ can contain at most $n$ linear independent vectors, so if $S \subset (R^n)^k = \{s_1, s_2, \ldots s_k\}$ where $k > n$, then some $s_i$ is a linear combination of the other $s_j$. Supposing without loss of generality that $s_1$ is this vector, by Lemma 8.3.10 $f(S) = f(\sum_{j > 1}a_js_j, s_2, \ldots s_n) = f(a_2s_2, s_2, \ldots s_n) = a_2f(s_2, s_2, \ldots s_n) = a_20 = 0$ for any multilinear alternating map $f$. 
\item Formula 8.3.1 for the determinant gives a multilinear nonzero alternating function when $k = n$, and when $k < n$ the same formula applied to $k$ is still alternating and multilinear.
\item If they were isomorphic, then a multilinear function on one would be multilinear on the other, but only the ones where $k \le n$ have multilinear functions. 
\ee
\item -108, by my calculations.
\item If the $k$th column and $l$th column of $A$ are the same, then $\det A_{ij}$ is 0 except when $j = k$ or $j=l$, because otherwise $A_{ij}$ contains both columns $k$ and $l$ and therefore has determinant 0. And the permutation that gets $A_{ik}$ from $A_{ij}$ is the cyclic permutation on columns $a_m$ for $k \le m \le l$ (assuming WLG that $l > m$), which has sign $(-1)^{l - k + 1}$, so $(-1)^{i + k}\det A_{ik} + (-1)^{i + l}\det A_{il} =  (-1)^{l + i + 1} \det A_{il} + (-1)^{l + i}\det A_{il} = 0$, and since those are the only nonzero terms in the cofactor expansion, it follows that the cofactor expansion is $0$ if two columns of $A$ are the same, so it's alternating.

For multilinearity, let $A_1 = (a_1, a_2, \ldots a_k, \ldots, a_n), A_2 = (a_1, a_2 ,\ldots a_{k}', \ldots a_n), A_3 = (a_1, a_2 ,\ldots a_k + a_{k}', \ldots a_n)$, then cofactor($A_3) = \sum_{j \not = k} (-1)^{i + j}a_{ij}\det A_{3_{ij}} + (-1)^{i + k}\det A_{3_{ik}}(a_k + a_k')$, so for the terms with $j \not = k$, $\det A_{3_{ij}} = \det A_{1_{ij}} + \det A_{2_{ij}}$ since the determinant is multilinear, while $\det A_{3_{ik}} = \det A_{1_{ik}} = \det A_{2_{ik}}$ since the only column that differs among them has been deleted. Taking these facts together, it follows that cofactor($A_3$) = cofactor($A_1$) + cofactor($A_2$), so the cofactor expansion is multilinear.

It is readily calculable that the cofactor expansion of the identity is one, so it is equal to the determinant by the above facts.
\item The determinant is invariant under transpose, so take the transpose of both sides of that equation and apply Exercise 5.
\item $\det(A)\inv \det(\tilde{A_j}) = \det(A\inv \tilde{A_j})$, and $A\inv \tilde{A_j}$ is the identity matrix except in the $j$th column, where it is $A\inv b$. Taking a cofactor expansion on the $j$th column of $A\inv \tilde{A_j}$, we get $\det(A\inv \tilde{A_j}) = (A\inv b)_j$. 
\ee
\section{Finitely Generated Modules over a PID, Part 1}
Summary: Every matrix $A$ over a PID is row-and-column equivalent to a diagonal matrix where each entry divides the following entries and then some zeroes, so $A = PAQ$ for invertible $P$ and $Q$, which means that any submodule of a free $R$-module has a basis $\{d_iv_i\}$, where $d_i$ divides $d_j$ for $i < j$ and the $v_i$ are linear independent over $R$.
\be
\item \be
\item Let $V = (v_1, \ldots v_n)$, then the $k$th row of $VA$ is $\sum_i A_{ik}v_i$, and so the $k$th row of $(VA)B$ is $\sum_iB_{ik}(\sum_j A_{jk}v_j)$, which if you distribute and calculate the coefficients of each $v_i$ you'll find that comes out to $\sum_i(\sum_jA_{ij}B_{jk})v_i$, whereas $(AB)_{ij} = \sum_lA_{il}B_{lj}$, so the $k$th row of $V(AB)$ is $\sum_i(\sum_jA_{ij}B_{jk})v_i$, which is the same.
\item Let $B = [v_1, \ldots v_n]A_j$, then the $j$th row of $B = \sum_i A_{ij}v_i$, a linear combination of the linearly independent $v_i$, so if the $j$th row of $B$ is 0 then the $j$th column of $A$ must be 0, and so if $B$ is 0 then $A$ is 0.
\ee
\item A submodule of a free $R$-module of rank 1 is isomorphic to an ideal of $R$, so it is generated by a single element since $R$ is a PID. So if $F$ is a free $R$-module of rank $n$, it has a basis $\{f_1, f_2, \ldots f_n\}$, so let $F' = \spn(\{f_1, f_2, \ldots f_{n-1}\})$. For a submodule $N$ of $F$, the submodule $N' = N \cap F'$ is generated by at most $n-1$ elements since $F'$ is and that's the induction hypothesis, whereas $N \setminus N'$ is generated by at most 1 element and that's the base case. And so any element $x \in N$ is equal to $x' + \pi_n(x)$, where $\pi_n(x)$ is the coefficient of $f_n$ in the expansion of $x$ and $x' \in N'$, since $x - \pi_n(x) \in N'$, so $N$ is generated by no more than $n$ elements.
\item These are known and attested properties of the Euclidean function, so I'll only be proving them for the new length thing.
\be
\item Each irreducible appearing in the factorization of $a$ and each irreducible appearing in the factorization of $b$ appear in the factorization of $ab$, so $|ab| \ge \max(|a|, |b|)$.
\item $a$ is an associate of $b \lra a = ub$ for a unit $u \ra$ the prime factorization of $a$ is a unit $u_a$ times a product of irreducibles $\prod_ip_i$ and the prime factorization of $b$ is $uu_a$ times $\prod_ip_i$. Since the product of units $uu_a$ is a unit, $|a| = |b|$ in this case.
\item If $a$ does not divide $b$ then at least one of $a$'s divisors $p_i$ does not divide $b$ so $p_i$ cannot be a factor of any common divisor $d$ of $a$ and $b$ so $|d| < |a|$ and by the transitive property of $<$ it follows that $|d| < |b|$.
\ee
\item The zero matrix $0 \in R$. If $A$ is row and column finite, so is $-A$ since $-0 = 0$. And $A + B$ has at most as many nonzero entries in each row and column as the sum of how many $A$ has in that row/column and how many $B$ has in that row/column, which will be finite if $A, B \in R$. So $R$ is a group. The diagonal matrix whose diagonal entries are 1 is a member of $R$, and it is a multiplicative identity. It's not commutative because matrix multiplication isn't commutative. And given $(A, B) \in R \oplus R$, let $C$ be the matrix whose $2i -1$ column is column $i$ of $A$ and $2i$ column is column $i$ of $B$ for $i \in \nats$, then the map $(A, B) \to C$ is an isomorphism $R \oplus R \to R$ since for $D \in R$ the $i$th column of $DA$ is $DA_i$ where $A_i$ is the $i$th column of $A$, while the $j$th column of $C$ is $DC_j$, so the isomorphism respects the ring product in $R$.
\item $\psi(\sum_ix_iv_i) = \sum_ix_i\psi(v_i) = \sum_ix_i(\sum_jA_{jk}w_k) = [w_1, \ldots w_m]A \left( \begin{array}{c} x_1 \\ x_2 \\ \ldots \\ x_n \end{array} \right)$.
\item The columns of $[w_1, \ldots w_m]A$ span range($\psi$), and $A = P\inv A'Q\inv$, so $[w_1, \ldots w_m]A = [w_1, \ldots w_m]P\inv A'Q\inv$. Let $[w'_1, \ldots w'_n] = [w_1, \ldots w_m]P\inv$, then $[w'_1, \ldots w'_n]$ is a basis of $W$ since $P\inv$ is invertible, and $[w'_1, \ldots w'_n] A' = [d_1w'_1, d_2w'_2, \ldots d_sw'_s, 0, 0, \ldots 0]$, which is a basis for range($\psi$) since it has the same span as $[w_1, \ldots w_m]A$ since $Q$ is invertible.
\item I ran the numbers and what came out of it was $P = \threemat{1}{0}{1}{1}{1}{-4}{1}{0}{2}, A' = \text{diag}(2, -2, -1), Q = \left( \begin{array}{cccc} 1 & 0 & 0 & 0 \\ -1 & 1& 0 & 3 \\ 0 & 0 &1 & 0 \\ 5 & 3 & 8 & 2 \end{array} \right)$, and the basis of $\ints^3$ that we want is the columns of $P\inv, \threemat{2}{0}{-1}{-6}{1}{5}{-1}{0}{1}$.
\item $\ker{A} = \ker{P\inv A'Q\inv} = \ker{A'Q\inv}$ since $P$ is invertible, so $A'x = 0 \lra x_j = 0$ for $1 \le j \le s$ since $A'$ is diagonal with $s$ nonzero entries on its diagonal, so we're looking for the vectors $x$ such that $Q\inv x = \left( \begin{array}{c} 0 \\ 0 \\ \ldots \\ 0 \\ y_{s+1} \\ y_{s+2} \\ \ldots \\ y_n \end{array} \right)$, which are of course $x = Q\left( \begin{array}{c} 0 \\ 0 \\ \ldots \\ 0 \\ y_{s+1} \\ y_{s+2} \\ \ldots \\ y_n \end{array} \right)$ for all $y_i$, aka the span of the last $n-s$ columns of $Q$. 
\item I screwed up the arithmetic on this one, but the point is you just find the Smith normal form of $A$ and the span of the last 2 columns of $Q$ is your null space.
\item $[w'_1, \ldots w'_s] = [w_1, \ldots w_s]D \lra [w'_1, \ldots w'_s]D\inv = [w_1, \ldots w_s]$, so the $w$ are a linear combination of the $w'$ as long as $D$ is invertible, so the $w'$ generate the $w$ and hence $N$.
\ee
\section{Finitely Generated Modules over a PID, Part 2}
Summary: Any finitely generated module $M = \spn\{x_1, \ldots x_n\}$ over a principal ideal domain $R$ is the sum of cyclic submodules $(\oplus_i R/(d_i)) \oplus R^k$, where the $d_i$ are the diagonal entries from the Smith normal form of $\sum_ir_if_i \to \sum_ir_ix_i$ where the $f_i$ are the basis of any free $R$-module of rank $n$. That's the "invariant factor decomposition". There's also the "primary decomposition", where any finitely generated torsion module with period $m = \prod_ip_i^{m_i}$ is equal to $\oplus_iM[p_i]$, where $M[p_i]$ is the set of elements $x$ with $p_i^kx = 0$ for some $k$. You can apply the primary decomposition to the summands of the invariant factor decomposition to get the "elementary divisor decomposition".
\be
\item Let $a \in \ann(M), r \in R, m \in M$, then $(ra)m = r(am) = r0 = 0$, so $\ann(M)$ is a ideal of $R$. $\ann(S) \subset \ann(RS)$ since $1 \in R$, and $\ann(RS) \subset \ann(S)$ since $\ann(RS) = R\ann(S)$ and $\ann(S)$ is an ideal.
\item $M / \mt = \{a + \mt: a \in M\}$ so $\exists a, r \in R: ra + \mt = 0 + \mt \lra ra \in \mt \lra \exists r' \in R: r'ra = 0 \lra (r'r)a = 0 \lra a \in \mt$, so $M/\mt$ is torsion free.
\item $B \cap \mt = \{0\}$ since if $\{b_1, \ldots b_n\}$ is a basis for $B$ then any $b \in B$ is equal to $\sum_ir_ib_i$ for $r_i \in R$, and if $rb = 0$ for some $r \in R$ then $r\sum_ir_ib_i = 0 \ra \sum_irr_ib_i = 0 \ra rr_i = 0$ for all $i$ since the $b_i$ are linear independent. So $B$ is torsion free and since $M = A + B$ we must have $\mt \subset A$, and since $A$ is a torsion group it follows that $A = \mt$.
\item $RB$ is an $R$-module with a basis $B$ so it's free, and for $a + RB \in M/RB$, if there doesn't exist an $r \in R$ such that $ra + RB = 0 + RB$, then $a$ is linearly independent of the vectors in the basis $B$, but $B$ is maximal so no such $a$ can exist. So $M/RB$ is a torsion module.
\item If $x \in J$ then $rx \not = 0$ for any $r \in R$ besides 0, since $J \subset R$ and $R$ is an integral domain, so $J$ is torsion free. If $a, b \in J$, then $0 = ab - ba \in RJ$, so any two elements of $J$ are linear independent over $R$. Because of this linear dependence between distinct elements of $J$, a linear independent subset of $J$ can contain at most one element, and if this single-element set were a basis of $J$ then $J$ would be a principal ideal, which it's defined not to be, so $J$ is not free.
\item Let $m/n + \ints \in \rats / \ints$, then $n(m/n) + \ints = m + \ints = 0 + \ints$, so $\rats / \ints$ is a torsion $\ints$-module. Any generating set of $\rats / \ints$ must contain at least $1/p$ for all primes $p$, since no sum or product of elements without denominator $p$ results in a fraction with denominator $p$ (by the primality of $p$ and the definitions of addition and multiplication in $\rats$). So $\rats / \ints$ is not finitely generated, since there are infinite primes. For any $n \not = 0 \in \ints, \frac{1}{2n} + \ints \in \rats/\ints$, and $n(\frac{1}{2n} + \ints) = \frac{1}{2} + \ints \not = 0 + \ints$, so $\ann(\rats / \ints) = \{0\}$.
\item \be
\item Suppose $p^k$ divides $a$, then $a = p^kq$ for some $q \in R$. Then let $f(p^{k-1}x + (a)) = qp^{k-1}x+ (a)$, so $f$ is a surjective homomorphism from $p^{k-1}M$ to $qp^{k-1}M$ with colonel $p^kM$. By the homomorphism theorem, then, $\frac{p^{k-1}M}{p^kM} \cong qp^{k-1}M$. And then $x \to qp^{k-1}x + (a)$ is a surjective homomorphism from $R$ to $qp^{k-1}(R/(a))$ with colonel $(p)$, so $qp^{k-1}M \cong R / (p)$, so by the transitive property of congruence it follows that $\frac{p^{k-1}M}{p^kM} \cong R / (p)$.

If $p^k$ does not divide $a$, then $p^ks + at = 1$ for some $s, t \in R$, so for any $p^{k-1}x + (a) \in M, p^{k-1}x + (a) = p^{k-1}(p^ks + at)x + (a) = p^kp^{k-1}sx + (a) \in p^kM$, so $p^{k-1}M \subset p^kM \ra \frac{p^{k-1}M}{p^kM} = \{0\}$.
\item $\frac{p^{k-1}M}{p^kM} = \frac{p^{k-1}(\oplus_iA_i)}{p^k(\oplus_iA_i)} = \frac{\oplus_ip^{k-1}A_i}{\oplus_ip^kA_i} = \oplus_i(\frac{p^{k-1}A_i}{p^kA_i})$ by Lemma 8.5.1, so since $A_i = R/(a_i)$ the direct summands here are $R/(p)$ when $p^k$ divides $a_i$ and 0 otherwise by part a. Since $X \oplus 0 \cong X$ for any set $X$, it follows that $\frac{p^{k-1}M}{p^kM} \cong (R/(p))^{m_k(p)}$. Since the set $\frac{p^{k-1}M}{p^kM}$ has no reference to or dependence on the $a_i$, only on $M$, it follows that the numbers $m_k(p)$ depend only on $M$ and not on the choice of direct summands $A_i$.
\item If the largest $m_k(p) = 1$, then $s=1$ and $a_1$ is the product of $\{p^k: k$ is the highest value with $m_k(p) = 1\}$, so $s$ and the $a_i$ are uniquely determined. So suppose that all maximal $m_k(p)$ less than $n$ uniquely identify their $s$ and $\{a_i\}$ up to units. Then let $a_n = \prod \{p^k: k$ is the highest value with $m_k(p) = 1\}$ and subtract 1 from all our $m_k(p)$ values and call them $m'_k(p)$, then the highest $m'_k(p)$ is $n-1$ and any product of $p^k: m'_k(p) \not = 0$ divides $a_n$, so the induction hypothesis uniquely determines $s$ and the rest of the $a_i$.
\ee
\item $M$ is annihilated by $\prod_ip_i^{m_i}$, so $p_i^{m_i}M$ is annihilated by $\prod_{j \not = i} p_j^{m_j}$, which is not divisible by $p_i$, so $(p_i^{m_i}M)[p_i] = \{0\}$, so no element of $M$ is annihilated by a higher power of $p_i$ than $m_i$, so for all $x \in M[p_i], p_i^{m_i}x = 0$.
\ee
\section{Rational Canonical Form}
\be
\item If $h(x) = \sum_ia_ix^i$, then let $g(x, y) = \sum_ia_i\frac{x^i - y^i}{x-y}$, then it is obvious that $(x - y)g(x, y) = h(x) - h(y)$, and it only remains to show that $\frac{x^i - y^i}{x-y} \in K[x]$ for all $i$. So let $q_0(x) = 1, q_n(x) = \sum_{k=0}^nx^ky^{n-k}$, then $(x-y)q_n(x) = \sum_{k=0}^nx^{k+1}y^{n-k} - \sum_{k=0}^nx^ky^{n-k+1}$ and all the terms in that difference cancel besides $x^{n+1} - y^{n+1}$ so $q_n(x) \in K[x]$ is $\frac{x^{n+1} - y^{n+1}}{x-y}$. 
\item Let $v \in V$ have the property $Wv = 0$, where $W$ is the matrix whose $i$th column is $w_i$. Let $F$ be the matrix where the $i$th column is $f_i$, then $W = F(xE_n - A) \ra F(xE_n-A)v = 0$, but $xE_n - A$'s columns are linearly independent since they have polynomials on the diagonal and nowhere else, and the columns of $F$ are linearly independent since the $f_i$ constitute a basis. So $v=0$, so $\ker{W} = \{0\}$, so the columns of $W$, i.e. the $w_i$, are linearly independent.
\item \be
\item $V \cong F/\ker{\Phi} = \oplus_i\frac{K[x]f_i}{K[x]g_i}$, where $f_i$ are the vectors in a basis for $F$ and $g_i$ are the vectors in a basis of $\ker{\Phi}$. The first $n-s$ summands are $\frac{K[x]y_i}{K[x]y_i} = 0$, so $V = \oplus_{i=n-s}^n\frac{K[x]z_i}{K[x]a_i(x)z_i}$, and $\frac{K[x]z_i}{K[x]a_i(x)z_i} \cong K[x] / (a_i(x))$ by the discussion on page 387, and $K[x] / (a_i(x)) = K[x] / \ann(v_i) \cong K[x]v_i = V_i$. So $V = \oplus_i V_i$.
\item The vectors $w_i = T^i(v_j)$ are linear independent because the annihilator of $V_j$ has degree $\delta_j$, and they span $V_j$ since there are $\delta_j$ of them. $T(w_i) = w_{i+1}$ for $0 < i < \delta_j-1$, and $T(w_{j-1}) = T^{\delta_j}(v_j) = a_j(T)(v_j) - \sum_{i=0}^{\delta_j-1}\alpha_iT^i(v_j)$ where $\alpha_i$ are the coefficients of $a_j$. Since $a_j(T)(v_j) = 0$, the result is simply $-\sum_{i=0}^{\delta_j-1}\alpha_iT^i(v_j)$, so the matrix of $T_{|V_j}$ with respect to this basis is the companion cube of $a_j$.
\ee
\item I got the rotational canonical form to be diag(1, 1, 1, $x-2$), and I was too lazy to do the basis thing. Have your computer do it.
\item $\chi_{SAS\inv} = \det{(xE - SAS\inv)} = \det{(S(xS\inv E - A)S\inv)} = \det{(S)}\det{(xSES\inv-A)}\det{(S\inv)} = \det{(xE-A)}$, so $\chi_A$ is a similarity invariant of matrices. So the characteristic polynomial of a linear transformation $T$ is the same without regard to underlying basis, so $\chi_T$ is well-defined and a similarity invariant for linear transformations.
\item Taking a cofactor expansion on the first row of an $n\times n$ matrix $xE - A$, the only term that has powers that go as high as $n-1$ is $a_{11}\det A_{11}$, since the matrices $A_{1j}$ for $j \not = 1$ all delete two of the monomials in $x$, so (inductively) the coefficient of $x^{n-1}$ in the characteristic polynomial of $A$ is the same as the coefficient of $x^{n-1}$ in $\prod_i(x-a_{ii})$, which is $-\sum_ia_{ii}$, so the trace is a similarity invariant. 
\item $\lambda$ is a root of $\chi_A \lra \det(\lambda E - A) = 0 \lra \exists v \in R^n: (\lambda E - A)v = 0 \lra \lambda Ev = Av \lra Av = \lambda v$.

$v$ is an eigenvector of $T \lra Tv = \lambda v \in Kv$ for some $\lambda \in K$.
\item $\bar{T}(v + v' + V_0) = T(v + v') + V_0 = T(v) + T(v') + V_0 = \bar{T}(v + V_0) + \bar{T}(v' + V_0)$, so $\bar{T}$ is a linear operator on $V / V_0$. $V \cong V/V_0 \oplus V_0$, so $(v_1, \ldots v_n)$ is a basis of $V$. Since $V_0$ is invariant under $T$, $T(v_i) \in \spn(v_1, \ldots v_k)$ for $1 \le i \le k$, so the first $k$ columns of the matrix of $T$ with respect to $(v_1, \ldots v_n)$ terminate in $n-k$ zeros, and the particular linear combination of the first $k$ vectors that gives $T(v_i)$ is given by multiplication by $A_1$, so the first block column of our matrix has the indicated form. Similarly the $j$th column of $A_2$ gives the projection of $T(v_j)$ onto the span of the last $n-k$ vectors for $k < j < n$, except we don't have that this space is invariant so $T(v_j)$ could have arbitrary components in the first $k$ vectors as well, which is where $B$ comes in.   
\item If $v$ is an eigenvector of $T$ then $K[x]v$ is a $T$-invariant subspace of $V$. If $n=1$, all matrices are upper triangular, so the matrix of $T$ is upper triangular with respect to any basis. So if all linear transformations with splitting characteristic polynomials on $R^k$ are upper-triangularizable for $k < n$, and we want to upper-triangularize a transformation $T$ on $R^n$ where $\chi_T$ splits, then we let $\lambda$ be some root of $\chi_T$, so that $\lambda$ is an eigenvalue of $T$ with eigenvector $v$, and let $V_0 = K[x]v$, then applying the previous exercise, we get a basis with respect to which the matrix of $T$ has the form $\twomat{\lambda}{B}{0}{A_2}$, and so by the induction hypothesis the restriction of $T$ to the last $n-1$ vectors in that basis is upper-triangularizable, so $T$ is upper-triangularizable.
\item The determinant of an upper-triangular matrix is the product of its diagonal entries, so $\chi_A = \prod_i(x-\lambda_i)$.
\item Let $v = \sum_{l=1}^ka_lv_l$, then $(T-\lambda_k)v = Tv - \lambda_kv = \sum_{l=1}^ka_lT(v_l) - \sum_{l=1}^ka_l\lambda_kv_l$. Since $T(v_k) = \lambda_kv_k$, this difference reduces to $\sum_{l=1}^{k-1}a_lT(v_l) - \sum_{l=1}^{k-1}a_l\lambda_kv_l \in V_{k-1}$ because $A'$ is upper triangular so $T(v_j)_k$ = 0 for $j < k$, and the $v_i$ are linear independent so no sum of them that doesn't involve $v_k$ adds up to something that does. So the foregoing covers both the base case and the induction step for the claim $\prod_{i=k}^n(T-\lambda_i)$ maps $V$ into $V_{k-1}$ for all $1 \le k \le n$, i.e. the whole claim. In particular, $\prod_{i=1}^n(T-\lambda_i)$ maps $V$ into $\{0\}$, but $\prod_{i=1}^n(T-\lambda_i) = \chi_T$, so it follows that $\chi_T(T) = 0$.
\ee
\section{Jordan Canonical Form}
\be
\item I got $J_3(5) \oplus J_1(2)$ and $S=\left( \begin{array}{cccc} 1 & -1/10 &51/100 & 0 \\ -3 & 3/10 & -53/100 & -1 \\ 0 & 0 & 1 & 0 \\ 10 & 0 & 0 & 4 \end{array} \right).$
\item Ditto.
\item Let $D$ be a diagonal matrix, then $A \sim D \lra  = SAS\inv$ for some $S$ $\lra$ the columns of $S$ form a basis with respect to which $A$ is diagonal $\lra As_i = D_{ii}s_i$ for $s_i$ a column of $S \lra s_i$ is an eigenvector of $A$ with eigenvalue $D_{ii}$. 
\item a $\lra$ b by the uniqueness of the Jordan canonical form since diagonal matrices are in Jordan canonical form and $A$ is similar to its Jordan canonical form. b $\lra$ c because if $A$ has a root $\lambda$ of dimension $d > 1$ then the Jordan block corresponding to that eigenvalue would be $J_d(\lambda)$ which is not diagonal, whereas $J_1(\lambda)$ is diagonal. c $\lra$ d because the minimal polynomial is the product of the elementary divisors.
\item Let $J$ be the Jordan form of $A$, then $A = SJS\inv$ for some matrix $S$, and $J = D + M$ where $D$ is the diagonal matrix of eigenvalues of $A$ and $M$ is 0 except at entries $M_{i+1i}$, i.e. the diagonal below the main diagonal, which is 0 or 1. Any matrix with the form of $M$ is nilpotent because the first row of all such $M$ is 0 and if the first $j$ rows of $M$ are 0 then the first $j+1$ rows of $M^2$ are 0, since row $j+1$ of $M^2$ can only be equal to row $j$ * 1 or row $j$ times 0, both of which are 0. If $M$ is nilpotent, then $SMS\inv$ is nilpotent, since $(SMS\inv)^2 = SMS\inv SMS\inv = SM^2S\inv$ and so forth. So $A = SJS\inv = S(D + M)S\inv = SDS\inv + SMS\inv = A_0 + N$ where $A_0$ is diagonalizable and $N$ is nilpotent. For the commutativity relation $A_0N = NA_0$, it suffices to show when $A$ is a Jordan block, so that $A_0 = \lambda E$ so $A_0N = \lambda EN = N\lambda E = NA_0$, since the general case involves only several distinct Jordan-block products. 
\item \be
\item If $\lambda$ is an eigenvalue of $N$ then there exists a vector $v$ such that $Nv = \lambda v \lra N^k v = \lambda^k v$ for all $k \in \nats$, but there exists an $m \in \nats$ such that $N^m = 0$, so $N^m v = 0v = \lambda^k v$ so $\lambda = 0$, so all the eigenvalues of $N$ are 0 so all the roots of the characteristic polynomial of $N$ are 0 so the polynomial is $x^n$. 
\item As shown above, all the eigenvalues of $N$ are 0, so all the Jordan blocks of the Jordan form of $N$ are $J_m(0)$.
\item Trace is a similarity invariant, $N$ is similar to its Jordan form, and the diagonal of the Jordan form of $N$ is all zeros, so the trace of $N$ is 0.
\item The Jordan form of $N$ as described in b is in rational canonical form, so by uniqueness of the rational canonical form it follows that the Jordan canonical form is the rational canonical form. 
\ee
\item They are all similar to the matrix $M$ whose elements are all zero besides $M_{ij}$ where $j = i+1$ and those $M_{ij}$ can be 0 or can be 1; all choices of 1s and 0s for this sub-diagonal correspond to possible nilpotent matrices.
\item \be
\item The characteristic polynomial of $S$ is $1-x^n = (x-1)(x-\xi)(x-\xi^2)\cdots(x-\xi^{n-1})$, so since the characteristic polynomial is a similarity invariant $S \sim D$.
\item Yes.
\ee
\item \be
\item The vector of ten ones is an eigenvector of $A$ with eigenvalue 10, and since the trace of $A$ is 10 and none of the eigenvalues of $A$ can be negative since all its entries are positive, it follows that the Jordan form of $A$ is the zero matrix but for one diagonal entry which is 10. Nine linear independent eigenvectors of eigenvalue 0 are given by the vectors with a 1 in position 1, zeros in eight of the nine remaining positions, and a -1 in the last remaining position.
\item $A^2 = 0$ if the characteristic of $K$ is 2 or 5, so $A$ is nilpotent. The rational canonical form of $A$ has one 1 in the diagonal below the main diagonal and zeros elsewhere. A basis with respect to which $A$ has this form is $[1, 0, 0, \ldots 0]$, $[1, 1, 1, \ldots 1], [1, 1, 0, 0, \ldots 0], [1, 0, 1, 0, 0, \ldots 0]$, and six other vectors with a 1 in the first position and one other position and 0s elsewhere in $\ints_2$. In $\ints_5$, the Jordan form is the same and the basis is the same except in the last eight vectors replace the 1 that's not in the first position with a 4.
\ee
\item \be
\item This is equal to the matrix from number 9 plus $3E$, so it has the same eigenvectors as that matrix with eigenvalues equal to $\lambda + 3$, where $\lambda$ is the eigenvalue found in 9. So the basis with respect to which it's in canonical form is the same, and the eigenvalues are $\lambda + 3$.
\item Under the same basis as in 9b, we get the Jordan canonical form, but the eigenvalues on the diagonal are now 1 mod 2 and 3 mod 5.
\ee
\ee
\end{document}